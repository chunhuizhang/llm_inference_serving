{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a48ecee-7236-42dd-a3e3-932fba0f95c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T00:46:15.115608Z",
     "iopub.status.busy": "2025-02-23T00:46:15.114967Z",
     "iopub.status.idle": "2025-02-23T00:46:15.126208Z",
     "shell.execute_reply": "2025-02-23T00:46:15.124194Z",
     "shell.execute_reply.started": "2025-02-23T00:46:15.115555Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a972a1-d7df-460e-953b-da9b60174a9f",
   "metadata": {},
   "source": [
    "- `gpu_memory_utilization`\n",
    "    - Weights\n",
    "    - activation\n",
    "    - kv cache\n",
    "- advanced parameters\n",
    "    - dynamic split fuse 默认 able（除非 disable）\n",
    "    - 开启之后，可以显著提升 TTFT（time to first token）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80920d8d-699f-4167-9576-960309569f0b",
   "metadata": {},
   "source": [
    "## prefill & decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fbe733-92b6-442e-bb74-c5ee8dd8b33d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T00:46:30.846460Z",
     "iopub.status.busy": "2025-02-23T00:46:30.845834Z",
     "iopub.status.idle": "2025-02-23T00:46:30.863793Z",
     "shell.execute_reply": "2025-02-23T00:46:30.861816Z",
     "shell.execute_reply.started": "2025-02-23T00:46:30.846397Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://pica.zhimg.com/v2-12657ad5334825239b580b86a7c7e0c4_1440w.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://pica.zhimg.com/v2-12657ad5334825239b580b86a7c7e0c4_1440w.jpg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a810b-b05d-4650-ad9d-ae4f6a61e135",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/14668411986\n",
    "\n",
    "- 在使用vllm的过程中，我们发现大语言模型输出token过程中，偶尔会出现有较长时间的等待才吐出下一个token的问题。\n",
    "    - 这是由于vllm服务是会在一个服务实例中，即做prefill又做decode。但同一时间只能执行prefill、decode其中一个，并且执行prefill的优先级高于decode。\n",
    "    - 所以在多并发情况下，会优先处理prefill而导致停滞decode，造成部分token会等待较长时间才输出的情况。\n",
    "- 如上图所示，假设最开始有A、B两个序列，他们都处在decode阶段。在A和B完成1次decode之后，来了C和D的两个请求。由于vllm是prefill优先的，所以它会先处理C和D的prefill，这就使得decode暂停了。等C和D的prefill完成了，A、B、C、D再同时做decode。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92cd66-860a-49c1-87e4-1550be616cd3",
   "metadata": {},
   "source": [
    "### chunked prefill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2590c8-0e9d-4f76-8b48-feaa26a103e7",
   "metadata": {},
   "source": [
    "- chunked-prefill，是修改了传统的推理服务里，非prefill即decode的方法。让prefill和decode能同时放在一个batch里做推理。对于比较长的请求序列，它的prefill无法再一个batch里执行完，它会做chunk切割，分在多个batch里完成。所以叫做chunked-prefill。其实也可以看出来，原来一条序列不切分直接进行prefill，和现在把它拆成了多个chunk，那么每个chunk去计算时，肯定要去读前一个chunk的KV cache，会额外多出一些开销。所以会稍微影响到TTFT，但是考虑到它对TPOT/TBT的更多提升，这样的开销还是可以接受的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e0960-076c-4664-aca3-f74a50f25178",
   "metadata": {},
   "source": [
    "### gpu-memory-utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994921d-b8e3-4424-8855-18887558efdf",
   "metadata": {},
   "source": [
    "- default: 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d051c55-43ad-452b-b958-23a2a6cfc171",
   "metadata": {},
   "source": [
    "### automatic prefix caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e7a84-fa95-4a84-887a-5591e3ee6708",
   "metadata": {},
   "source": [
    "- The core idea of PagedAttention is to partition the KV cache of each request into KV Blocks. Each block contains the attention keys and values for a fixed number of tokens. The PagedAttention algorithm allows these blocks to be stored in non-contiguous physical memory so that we can eliminate memory fragmentation by allocating the memory on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c72dde-1a46-4bf8-bdbe-0b7f94a04c8d",
   "metadata": {},
   "source": [
    "## generate vs. chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e491072a-5dd9-4181-8b9e-20f73fd5cb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:48:03.431676Z",
     "iopub.status.busy": "2025-02-13T13:48:03.431331Z",
     "iopub.status.idle": "2025-02-13T13:48:32.304573Z",
     "shell.execute_reply": "2025-02-13T13:48:32.303638Z",
     "shell.execute_reply.started": "2025-02-13T13:48:03.431652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:48:04 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 02-13 21:48:05 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 02-13 21:48:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-13 21:48:06 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883a261b37947ff919dc7d90ba1cd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:48:07 model_runner.py:1071] Loading model weights took 0.9228 GB\n",
      "INFO 02-13 21:48:08 gpu_executor.py:122] # GPU blocks: 100627, # CPU blocks: 21845\n",
      "INFO 02-13 21:48:08 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 49.13x\n",
      "INFO 02-13 21:48:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-13 21:48:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-13 21:48:29 model_runner.py:1530] Graph capturing finished in 18 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "llm = LLM(model='Qwen/Qwen2.5-0.5B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3521490b-5409-4d41-bb22-dce75b0fbcec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:48:51.984256Z",
     "iopub.status.busy": "2025-02-13T13:48:51.983599Z",
     "iopub.status.idle": "2025-02-13T13:48:52.117076Z",
     "shell.execute_reply": "2025-02-13T13:48:52.114903Z",
     "shell.execute_reply.started": "2025-02-13T13:48:51.984205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s, est. speed input: 18.35 toks/s, output: 146.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "gen_output = llm.generate('hello?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc816f08-b51e-434e-ac18-ce7ab29d3f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:49:26.060728Z",
     "iopub.status.busy": "2025-02-13T13:49:26.060096Z",
     "iopub.status.idle": "2025-02-13T13:49:26.073396Z",
     "shell.execute_reply": "2025-02-13T13:49:26.070956Z",
     "shell.execute_reply.started": "2025-02-13T13:49:26.060679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello?', ' Help please?\\\\n\\\\nTopic: SOy dhy so thy sis? D')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_output[0].prompt, gen_output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffb988-c97f-43ea-a524-63fadd7c491f",
   "metadata": {},
   "source": [
    "### chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9eb41-9d9d-45cc-9395-7e462edb6a53",
   "metadata": {},
   "source": [
    "- 送给 model 前向 inference 的是这里的 prompt\n",
    "    - 与 model 训练时的 chat ml template 相对；\n",
    "    - 见代码中的 `chat_output[0].prompt`\n",
    "- 这里可以看到对 instruct model 对 chat 的兼容性更好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88df5648-6f6b-468c-94a6-cb013ebd9a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:41.134702Z",
     "iopub.status.busy": "2025-02-13T13:50:41.134014Z",
     "iopub.status.idle": "2025-02-13T13:50:41.145522Z",
     "shell.execute_reply": "2025-02-13T13:50:41.143233Z",
     "shell.execute_reply.started": "2025-02-13T13:50:41.134647Z"
    }
   },
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c389cca-8db7-4ebc-be29-a7b08ca7ca95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:45.526616Z",
     "iopub.status.busy": "2025-02-13T13:50:45.525827Z",
     "iopub.status.idle": "2025-02-13T13:50:45.681987Z",
     "shell.execute_reply": "2025-02-13T13:50:45.680581Z",
     "shell.execute_reply.started": "2025-02-13T13:50:45.526531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s, est. speed input: 489.60 toks/s, output: 163.16 toks/s]\n"
     ]
    }
   ],
   "source": [
    "chat_output = llm.chat(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74893053-7317-493e-8c8e-399842022365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:55.453901Z",
     "iopub.status.busy": "2025-02-13T13:50:55.453201Z",
     "iopub.status.idle": "2025-02-13T13:50:55.465791Z",
     "shell.execute_reply": "2025-02-13T13:50:55.463240Z",
     "shell.execute_reply.started": "2025-02-13T13:50:55.453845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHello! How can I assist you today?<|im_end|>\\n<|im_start|>user\\nWrite an essay about the importance of higher education.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_output[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c594c5e3-0cde-40eb-8175-d5cb839f71de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:51:52.257356Z",
     "iopub.status.busy": "2025-02-13T13:51:52.256679Z",
     "iopub.status.idle": "2025-02-13T13:51:52.270224Z",
     "shell.execute_reply": "2025-02-13T13:51:52.267767Z",
     "shell.execute_reply.started": "2025-02-13T13:51:52.257305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Higher education is crucial in today's world for many reasons. It provides a foundation\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd35f6-ad95-439b-9517-ea3a851096cb",
   "metadata": {},
   "source": [
    "## quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875d6e5-4388-40fa-a58e-46783dd8f6e6",
   "metadata": {},
   "source": [
    "- vllm vs. ollama\n",
    "    - vllm: awq, gptq\n",
    "    - ollama: gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c69d7-b4b6-4f5c-924d-f6c8d4c82311",
   "metadata": {},
   "source": [
    "## benchmark_serving.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7600d-b863-4b13-9778-ace59ac0d71f",
   "metadata": {},
   "source": [
    "- benchmark metircs\n",
    "    - ttft: time to first token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
