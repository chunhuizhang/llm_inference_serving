{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a972a1-d7df-460e-953b-da9b60174a9f",
   "metadata": {},
   "source": [
    "- `gpu_memory_utilization`\n",
    "    - Weights\n",
    "    - activation\n",
    "    - kv cache\n",
    "- advanced parameters\n",
    "    - dynamic split fuse 默认 able（除非 disable）\n",
    "    - 开启之后，可以显著提升 TTFT（time to first token）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c72dde-1a46-4bf8-bdbe-0b7f94a04c8d",
   "metadata": {},
   "source": [
    "## generate vs. chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e491072a-5dd9-4181-8b9e-20f73fd5cb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:48:03.431676Z",
     "iopub.status.busy": "2025-02-13T13:48:03.431331Z",
     "iopub.status.idle": "2025-02-13T13:48:32.304573Z",
     "shell.execute_reply": "2025-02-13T13:48:32.303638Z",
     "shell.execute_reply.started": "2025-02-13T13:48:03.431652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:48:04 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 02-13 21:48:05 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 02-13 21:48:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-13 21:48:06 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883a261b37947ff919dc7d90ba1cd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:48:07 model_runner.py:1071] Loading model weights took 0.9228 GB\n",
      "INFO 02-13 21:48:08 gpu_executor.py:122] # GPU blocks: 100627, # CPU blocks: 21845\n",
      "INFO 02-13 21:48:08 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 49.13x\n",
      "INFO 02-13 21:48:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-13 21:48:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-13 21:48:29 model_runner.py:1530] Graph capturing finished in 18 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "llm = LLM(model='Qwen/Qwen2.5-0.5B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3521490b-5409-4d41-bb22-dce75b0fbcec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:48:51.984256Z",
     "iopub.status.busy": "2025-02-13T13:48:51.983599Z",
     "iopub.status.idle": "2025-02-13T13:48:52.117076Z",
     "shell.execute_reply": "2025-02-13T13:48:52.114903Z",
     "shell.execute_reply.started": "2025-02-13T13:48:51.984205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s, est. speed input: 18.35 toks/s, output: 146.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "gen_output = llm.generate('hello?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc816f08-b51e-434e-ac18-ce7ab29d3f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:49:26.060728Z",
     "iopub.status.busy": "2025-02-13T13:49:26.060096Z",
     "iopub.status.idle": "2025-02-13T13:49:26.073396Z",
     "shell.execute_reply": "2025-02-13T13:49:26.070956Z",
     "shell.execute_reply.started": "2025-02-13T13:49:26.060679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello?', ' Help please?\\\\n\\\\nTopic: SOy dhy so thy sis? D')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_output[0].prompt, gen_output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffb988-c97f-43ea-a524-63fadd7c491f",
   "metadata": {},
   "source": [
    "### chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9eb41-9d9d-45cc-9395-7e462edb6a53",
   "metadata": {},
   "source": [
    "- 送给 model 前向 inference 的是这里的 prompt\n",
    "    - 与 model 训练时的 chat ml template 相对；\n",
    "    - 见代码中的 `chat_output[0].prompt`\n",
    "- 这里可以看到对 instruct model 对 chat 的兼容性更好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88df5648-6f6b-468c-94a6-cb013ebd9a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:41.134702Z",
     "iopub.status.busy": "2025-02-13T13:50:41.134014Z",
     "iopub.status.idle": "2025-02-13T13:50:41.145522Z",
     "shell.execute_reply": "2025-02-13T13:50:41.143233Z",
     "shell.execute_reply.started": "2025-02-13T13:50:41.134647Z"
    }
   },
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c389cca-8db7-4ebc-be29-a7b08ca7ca95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:45.526616Z",
     "iopub.status.busy": "2025-02-13T13:50:45.525827Z",
     "iopub.status.idle": "2025-02-13T13:50:45.681987Z",
     "shell.execute_reply": "2025-02-13T13:50:45.680581Z",
     "shell.execute_reply.started": "2025-02-13T13:50:45.526531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s, est. speed input: 489.60 toks/s, output: 163.16 toks/s]\n"
     ]
    }
   ],
   "source": [
    "chat_output = llm.chat(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74893053-7317-493e-8c8e-399842022365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:50:55.453901Z",
     "iopub.status.busy": "2025-02-13T13:50:55.453201Z",
     "iopub.status.idle": "2025-02-13T13:50:55.465791Z",
     "shell.execute_reply": "2025-02-13T13:50:55.463240Z",
     "shell.execute_reply.started": "2025-02-13T13:50:55.453845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHello! How can I assist you today?<|im_end|>\\n<|im_start|>user\\nWrite an essay about the importance of higher education.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_output[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c594c5e3-0cde-40eb-8175-d5cb839f71de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:51:52.257356Z",
     "iopub.status.busy": "2025-02-13T13:51:52.256679Z",
     "iopub.status.idle": "2025-02-13T13:51:52.270224Z",
     "shell.execute_reply": "2025-02-13T13:51:52.267767Z",
     "shell.execute_reply.started": "2025-02-13T13:51:52.257305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Higher education is crucial in today's world for many reasons. It provides a foundation\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd35f6-ad95-439b-9517-ea3a851096cb",
   "metadata": {},
   "source": [
    "## quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875d6e5-4388-40fa-a58e-46783dd8f6e6",
   "metadata": {},
   "source": [
    "- vllm vs. ollama\n",
    "    - vllm: awq, gptq\n",
    "    - ollama: gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c69d7-b4b6-4f5c-924d-f6c8d4c82311",
   "metadata": {},
   "source": [
    "## benchmark_serving.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7600d-b863-4b13-9778-ace59ac0d71f",
   "metadata": {},
   "source": [
    "- benchmark metircs\n",
    "    - ttft: time to first token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
